{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.parse import CoreNLPParser\n",
    "import stanza\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "from book_processor.Book import dump_books_by_genre\n",
    "from notebook_utils.constants import PROJ_ROOT, NEW_GENRES\n",
    "from notebook_utils.data_loader import load_all_books\n",
    "\n",
    "# nlp = stanza.Pipeline(\"en\", processors=\"tokenize\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clausal = [\"S\", \"SBAR\", \"SBARQ\", \"SINV\", \"SQ\"]\n",
    "phrasal = [\"ADJP\", \"ADVP\", \"CONJP\", \"FRAG\", \"INTJ\", \"LST\", \"NAC\", \"NP\", \"NX\", \"PP\", \"PRN\", \"PRT\", \"QP\", \"RRC\", \"UCP\", \"VP\",\n",
    "           \"WHADJP\", \"WHAVP\", \"WHNP\", \"WHPP\", \"X\"]\n",
    "\n",
    "\n",
    "def traverse(tree, tags):\n",
    "    for subtree in tree:\n",
    "        if not isinstance(subtree, str):\n",
    "            label = subtree.label()\n",
    "            if label in clausal:\n",
    "                if label in tags[\"clausal\"].keys():\n",
    "                    tags[\"clausal\"][label] += 1\n",
    "                else:\n",
    "                    tags[\"clausal\"][label] = 1\n",
    "            elif label in phrasal:\n",
    "                if label in tags[\"phrasal\"].keys():\n",
    "                    tags[\"phrasal\"][label] += 1\n",
    "                else:\n",
    "                    tags[\"phrasal\"][label] = 1\n",
    "            elif label not in string.punctuation:\n",
    "                if label in tags[\"pos\"].keys():\n",
    "                    tags[\"pos\"][label] += 1\n",
    "                else:\n",
    "                    tags[\"pos\"][label] = 1\n",
    "            if len(subtree) > 1:\n",
    "                traverse(subtree, tags)\n",
    "\n",
    "\n",
    "# clausal_data = {genre: [] for genre in NEW_GENRES}\n",
    "# phrasal_data = {genre: [] for genre in NEW_GENRES}\n",
    "# pos_data = {genre: [] for genre in NEW_GENRES}\n",
    "\n",
    "all_books = load_all_books()\n",
    "# all_tags = {genre: {\"clausal\": [], \"phrasal\": [], \"pos\": []} for genre in NEW_GENRES}\n",
    "all_tags = {genre: {\"G\": [], \"GG\": [], \"g\": [], \"gG\": []} for genre in NEW_GENRES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5acb7903a1342cc95c20a2460282da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=799000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######## Dumping books as pickle object to C:\\Users\\hgore\\OneDrive - Fordham University\\Documents\\Fordham\\Year 2\\Research\\Projects\\CSE-538-Project-master\\data\\books_by_genre\\all_Adventure_Stories_books.txt ########\n",
      "\n",
      "######## Dumping books as pickle object to C:\\Users\\hgore\\OneDrive - Fordham University\\Documents\\Fordham\\Year 2\\Research\\Projects\\CSE-538-Project-master\\data\\books_by_genre\\all_Fiction_books.txt ########\n",
      "\n",
      "######## Dumping books as pickle object to C:\\Users\\hgore\\OneDrive - Fordham University\\Documents\\Fordham\\Year 2\\Research\\Projects\\CSE-538-Project-master\\data\\books_by_genre\\all_Historical_Fiction_books.txt ########\n",
      "\n",
      "######## Dumping books as pickle object to C:\\Users\\hgore\\OneDrive - Fordham University\\Documents\\Fordham\\Year 2\\Research\\Projects\\CSE-538-Project-master\\data\\books_by_genre\\all_Love_Stories_books.txt ########\n",
      "\n",
      "######## Dumping books as pickle object to C:\\Users\\hgore\\OneDrive - Fordham University\\Documents\\Fordham\\Year 2\\Research\\Projects\\CSE-538-Project-master\\data\\books_by_genre\\all_Mystery_books.txt ########\n",
      "\n",
      "######## Dumping books as pickle object to C:\\Users\\hgore\\OneDrive - Fordham University\\Documents\\Fordham\\Year 2\\Research\\Projects\\CSE-538-Project-master\\data\\books_by_genre\\all_Poetry_books.txt ########\n",
      "\n",
      "######## Dumping books as pickle object to C:\\Users\\hgore\\OneDrive - Fordham University\\Documents\\Fordham\\Year 2\\Research\\Projects\\CSE-538-Project-master\\data\\books_by_genre\\all_Science_Fiction_books.txt ########\n",
      "\n",
      "######## Dumping books as pickle object to C:\\Users\\hgore\\OneDrive - Fordham University\\Documents\\Fordham\\Year 2\\Research\\Projects\\CSE-538-Project-master\\data\\books_by_genre\\all_Short_Stories_books.txt ########\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bar_length = sum(len(all_books[genre]) for genre in NEW_GENRES) * 1000\n",
    "\n",
    "parser = CoreNLPParser()\n",
    "\n",
    "with tqdm(total=bar_length) as pbar:\n",
    "    for genre in NEW_GENRES:\n",
    "        for i, book in enumerate(all_books[genre]):\n",
    "            pbar.set_postfix_str(f\" -- {genre} -- [{i + 1}/{len(all_books[genre])}] \")\n",
    "            if book.book_number == \"19513\" or book.book_number == \"19640\" or book.book_number == \"19678\" \\\n",
    "                    or book.book_number == \"19782\" or book.book_number == \"19836\" or book.book_number == \"22326\" \\\n",
    "                    or book.book_number == \"1322\":\n",
    "                pbar.update(1000)\n",
    "                continue\n",
    "\n",
    "            # if isinstance(book.first_1k_sentences, list):\n",
    "            #     first_1k = \"\".join(book.first_1k_sentences)\n",
    "            # first_1k = re.sub(\"_\", \"\", first_1k)\n",
    "            # first_1k = re.sub(\"chapter ([ivx]+\\\\s+|\\\\w+\\\\s+?)\", \"\", first_1k, re.IGNORECASE)\n",
    "\n",
    "            try:\n",
    "                # doc = nlp(first_1k)\n",
    "                # sentences = [sent.text for sent in doc.sentences[:1000]]\n",
    "                sentences = all_books[genre][i].first_1k_sentences\n",
    "\n",
    "                # all_books[genre][i].first_1k_sentences = sentences\n",
    "                # all_tags[genre][all_books[genre][i].book_number] = []\n",
    "                book_tags = []\n",
    "\n",
    "                for sentence in sentences:\n",
    "                    results = [r for r in parser.raw_parse(sentence, properties={\"annotators\": \"tokenize,ssplit,pos,parse\"})]\n",
    "                    sent_tags = {\"clausal\": {}, \"phrasal\": {}, \"pos\": {}}\n",
    "                    traverse(results[0], sent_tags)\n",
    "\n",
    "                    book_tags.append(sent_tags)\n",
    "\n",
    "                    # c = {\"Book #\": all_books[genre][i].book_number, \"@Genre\": genre}\n",
    "                    # p = {\"Book #\": all_books[genre][i].book_number, \"@Genre\": genre}\n",
    "\n",
    "                    # for k, v in sent_tags[\"clausal\"].items():\n",
    "                    #     c.update({k: v})\n",
    "                    # for k, v in sent_tags[\"phrasal\"].items():\n",
    "                    #     p.update({k: v})\n",
    "\n",
    "                    pbar.update(1)\n",
    "\n",
    "                if len(all_books[genre][i].first_1k_sentences) < 1000:\n",
    "                    pbar.update(1000 - len(all_books[genre][i].first_1k_sentences))\n",
    "\n",
    "                counts = {\"clausal\": sum([Counter(book_tags[j][\"clausal\"]) for j in range(len(book_tags))], Counter()),\n",
    "                          \"phrasal\": sum([Counter(book_tags[j][\"phrasal\"]) for j in range(len(book_tags))], Counter()),\n",
    "                          \"pos\": sum([Counter(book_tags[j][\"pos\"]) for j in range(len(book_tags))], Counter())}\n",
    "\n",
    "                full_book_data = {\"clausal\": {\"Book #\": all_books[genre][i].book_number, \"@Genre\": genre},\n",
    "                                  \"phrasal\": {\"Book #\": all_books[genre][i].book_number, \"@Genre\": genre},\n",
    "                                  \"pos\": {\"Book #\": all_books[genre][i].book_number, \"@Genre\": genre}}\n",
    "                \n",
    "                full_book_data[\"clausal\"].update({k: v for k, v in counts[\"clausal\"].items() if k != \"''\" and k != \"``\" and k not in string.punctuation})\n",
    "                full_book_data[\"clausal\"][\"@Outcome\"] = all_books[genre][i].success\n",
    "                \n",
    "                full_book_data[\"phrasal\"].update({k: v for k, v in counts[\"phrasal\"].items() if k != \"''\" and k != \"``\" and k not in string.punctuation})\n",
    "                full_book_data[\"phrasal\"][\"@Outcome\"] = all_books[genre][i].success\n",
    "                \n",
    "                full_book_data[\"pos\"].update({k: v for k, v in counts[\"pos\"].items() if k != \"''\" and k != \"``\" and k not in string.punctuation})\n",
    "                full_book_data[\"pos\"][\"@Outcome\"] = all_books[genre][i].success\n",
    "                \n",
    "                all_books[genre][i].first_1k_pos_counts = full_book_data\n",
    "                \n",
    "                for tag_type in [\"clausal\", \"phrasal\", \"pos\"]:\n",
    "                    all_tags[genre][tag_type].append(full_book_data[tag_type])\n",
    "\n",
    "            except (AssertionError, RuntimeError) as e:\n",
    "                print(f\"{genre}, {book.success}, {book.book_number}\")\n",
    "                pbar.update(1000)\n",
    "                continue\n",
    "\n",
    "        # dump_books_by_genre(all_books[genre], genre)\n",
    "\n",
    "        for tag_type in [\"clausal\", \"phrasal\", \"pos\"]:\n",
    "            with open(str(PROJ_ROOT.joinpath(\"data\", f\"{genre}_{tag_type}_data.txt\")), \"wb+\") as f:\n",
    "                try:\n",
    "                    pickle.dump(all_tags[genre][tag_type], f)\n",
    "                except MemoryError:\n",
    "                    print(f\"There was a MemoryError when dumping {genre}_{tag_type}_data\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
